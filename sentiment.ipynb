{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 SENT50 (10Hr?)",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmbbiaQvpsmv",
        "colab_type": "text"
      },
      "source": [
        "### Make the stuff do the thing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3tbyD6upsC_",
        "colab_type": "code",
        "outputId": "db427057-21c0-464b-b967-edeb39d95b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "# import model, sample, encoder\n",
        "!pip install gpt-2-simple \n",
        "import gpt_2_simple as gpt2\n",
        "\n",
        "model_name = \"345M\" # The GPT-2 model we're using\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name) # Download the model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/3d/ee/bb41a7cc57e0626a0dfeea0f8fedc21e255103c888f5cab5e1f7fb00380b/gpt_2_simple-0.7.tar.gz\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.17.4)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7-cp36-none-any.whl size=23557 sha256=8054885574e76102e3604e91c9cdbac9ae6cc48c7a104c8c6eb94edf5d3bc335\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/7f/89/1253cc7ae7fd1cdf130fa146ab17314fd2a5a6d48ccf21dec5\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: regex, toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7 regex-2019.11.1 toposort-1.5\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 159Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 89.8Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 252Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:18, 74.9Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 427Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 94.6Mit/s]                                                \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 145Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Z555-noSQ-",
        "colab_type": "text"
      },
      "source": [
        "### Finetune Model\n",
        "Training the model on a custom dataset (50 positive and 50 negative samples from the stanford sentiment analysis dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vcKv6sNmaiZ",
        "colab_type": "code",
        "outputId": "8d0e2e05-7c18-4d9f-9193-80f877c3046e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "file_name = \"sentiment50.txt\" # where the dataset is located\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess,\n",
        "              file_name,\n",
        "              model_name=model_name,\n",
        "              steps=400)   # steps is max number of training steps\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/345M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 29942 tokens\n",
            "Training...\n",
            "[1 | 77.48] loss=3.39 avg=3.39\n",
            "[2 | 141.33] loss=3.50 avg=3.44\n",
            "[3 | 205.49] loss=3.03 avg=3.31\n",
            "[4 | 269.56] loss=3.51 avg=3.36\n",
            "[5 | 333.40] loss=3.38 avg=3.36\n",
            "[6 | 397.17] loss=3.40 avg=3.37\n",
            "[7 | 461.15] loss=2.65 avg=3.26\n",
            "[8 | 525.45] loss=3.65 avg=3.31\n",
            "[9 | 590.01] loss=2.86 avg=3.26\n",
            "[10 | 654.03] loss=3.41 avg=3.28\n",
            "[11 | 717.95] loss=3.17 avg=3.27\n",
            "[12 | 781.84] loss=3.06 avg=3.25\n",
            "[13 | 845.87] loss=2.16 avg=3.16\n",
            "[14 | 909.98] loss=2.51 avg=3.11\n",
            "[15 | 974.17] loss=4.21 avg=3.19\n",
            "[16 | 1038.08] loss=2.84 avg=3.16\n",
            "[17 | 1102.12] loss=3.02 avg=3.15\n",
            "[18 | 1166.25] loss=2.46 avg=3.11\n",
            "[19 | 1230.57] loss=2.64 avg=3.09\n",
            "[20 | 1294.70] loss=2.26 avg=3.04\n",
            "[21 | 1358.72] loss=3.52 avg=3.07\n",
            "[22 | 1422.63] loss=3.58 avg=3.09\n",
            "[23 | 1486.45] loss=2.26 avg=3.05\n",
            "[24 | 1550.27] loss=3.34 avg=3.06\n",
            "[25 | 1614.11] loss=2.91 avg=3.06\n",
            "[26 | 1677.34] loss=3.09 avg=3.06\n",
            "[27 | 1739.31] loss=3.17 avg=3.06\n",
            "[28 | 1803.11] loss=3.00 avg=3.06\n",
            "[29 | 1867.54] loss=2.75 avg=3.05\n",
            "[30 | 1931.93] loss=2.72 avg=3.04\n",
            "[31 | 1996.03] loss=2.74 avg=3.02\n",
            "[32 | 2060.27] loss=2.90 avg=3.02\n",
            "[33 | 2124.58] loss=3.19 avg=3.03\n",
            "[34 | 2188.55] loss=2.31 avg=3.00\n",
            "[35 | 2252.79] loss=3.19 avg=3.01\n",
            "[36 | 2317.19] loss=2.69 avg=3.00\n",
            "[37 | 2381.37] loss=2.98 avg=3.00\n",
            "[38 | 2445.47] loss=2.16 avg=2.97\n",
            "[39 | 2509.21] loss=2.27 avg=2.95\n",
            "[40 | 2573.11] loss=2.51 avg=2.94\n",
            "[41 | 2636.78] loss=1.97 avg=2.91\n",
            "[42 | 2700.51] loss=2.19 avg=2.89\n",
            "[43 | 2764.51] loss=1.85 avg=2.86\n",
            "[44 | 2828.34] loss=2.70 avg=2.85\n",
            "[45 | 2892.01] loss=1.81 avg=2.82\n",
            "[46 | 2955.83] loss=1.44 avg=2.79\n",
            "[47 | 3019.67] loss=1.87 avg=2.76\n",
            "[48 | 3083.80] loss=1.49 avg=2.73\n",
            "[49 | 3147.47] loss=1.14 avg=2.69\n",
            "[50 | 3211.13] loss=1.92 avg=2.67\n",
            "[51 | 3274.75] loss=1.55 avg=2.64\n",
            "[52 | 3338.47] loss=3.92 avg=2.67\n",
            "[53 | 3402.43] loss=1.96 avg=2.65\n",
            "[54 | 3466.06] loss=1.28 avg=2.62\n",
            "[55 | 3529.70] loss=1.33 avg=2.59\n",
            "[56 | 3593.50] loss=3.33 avg=2.61\n",
            "[57 | 3657.09] loss=3.46 avg=2.63\n",
            "[58 | 3721.02] loss=2.17 avg=2.62\n",
            "[59 | 3784.67] loss=2.59 avg=2.62\n",
            "[60 | 3848.87] loss=1.68 avg=2.60\n",
            "[61 | 3913.12] loss=1.82 avg=2.58\n",
            "[62 | 3977.53] loss=1.70 avg=2.56\n",
            "[63 | 4042.08] loss=2.81 avg=2.57\n",
            "[64 | 4106.86] loss=2.34 avg=2.56\n",
            "[65 | 4171.46] loss=2.72 avg=2.56\n",
            "[66 | 4235.83] loss=2.31 avg=2.56\n",
            "[67 | 4300.14] loss=3.29 avg=2.57\n",
            "[68 | 4364.17] loss=2.31 avg=2.57\n",
            "[69 | 4428.51] loss=1.26 avg=2.54\n",
            "[70 | 4492.65] loss=1.76 avg=2.53\n",
            "[71 | 4556.81] loss=3.33 avg=2.54\n",
            "[72 | 4621.07] loss=2.40 avg=2.54\n",
            "[73 | 4684.80] loss=1.21 avg=2.51\n",
            "[74 | 4748.46] loss=1.42 avg=2.49\n",
            "[75 | 4812.36] loss=0.66 avg=2.46\n",
            "[76 | 4876.00] loss=1.31 avg=2.44\n",
            "[77 | 4939.70] loss=1.15 avg=2.41\n",
            "[78 | 5003.37] loss=1.14 avg=2.39\n",
            "[79 | 5067.16] loss=1.60 avg=2.38\n",
            "[80 | 5130.84] loss=0.88 avg=2.35\n",
            "[81 | 5194.72] loss=0.58 avg=2.32\n",
            "[82 | 5258.58] loss=1.62 avg=2.30\n",
            "[83 | 5320.88] loss=1.65 avg=2.29\n",
            "[84 | 5384.26] loss=0.90 avg=2.27\n",
            "[85 | 5448.41] loss=1.08 avg=2.25\n",
            "[86 | 5511.98] loss=1.41 avg=2.23\n",
            "[87 | 5575.69] loss=0.59 avg=2.20\n",
            "[88 | 5639.28] loss=2.54 avg=2.21\n",
            "[89 | 5702.78] loss=0.95 avg=2.19\n",
            "[90 | 5766.52] loss=0.88 avg=2.17\n",
            "[91 | 5830.34] loss=0.85 avg=2.15\n",
            "[92 | 5894.28] loss=1.18 avg=2.13\n",
            "[93 | 5957.98] loss=1.58 avg=2.12\n",
            "[94 | 6021.94] loss=1.60 avg=2.11\n",
            "[95 | 6086.18] loss=1.49 avg=2.10\n",
            "[96 | 6150.06] loss=1.80 avg=2.10\n",
            "[97 | 6213.80] loss=0.52 avg=2.07\n",
            "[98 | 6277.50] loss=1.10 avg=2.06\n",
            "[99 | 6341.40] loss=0.63 avg=2.03\n",
            "[100 | 6405.26] loss=0.71 avg=2.01\n",
            "======== SAMPLE 1 ========\n",
            " in the past few hundred years has created a society that is more like a medieval Europe than our own. Women are not respected; men are not valued; and even the most basic human rights are rarely granted for women. Men, on the other hand, are highly valued in society, and can expect to be treated with respect. This in turn, makes it easier for society to contain the spread of crime and disorder. It also helps to prevent people becoming depressed because they can never trust anybody. In addition, it makes life easier in a society where everyone is expected to keep their promises. Finally, it teaches us all to treat one another with kindness and respect, which should go a long way in our struggle to maintain a civilized society. Sincerely, Martin Luther King Jr. \"Live Free or Die! Die Young!\"\n",
            "\n",
            "Free speech, not hate speech, is what motivates us as a society. When we allow speech that is false, hateful, and damaging, we are enabling those who publish it. When hate speech is suppressed, when it interferes with the ability of others to live a complete and productive life, people who own that hate speech are abusing their power. When it interferes with the ability of others to earn a living, when it compromises our ability to fight crime, the ability to live a long and productive life, when it compromises the social order that we all want to sustain, then there is always someone to blame. When a society supports hate speech, then it always supports the very hate speech that it is trying to quell. When a nation allows its citizens to be violent and destructive and to manipulate one another in ways that are beyond human understanding, then there will always be perpetrators who will use violence to maintain their control. That is exactly what is happening in this country today. That is precisely why I said, Live Free or Die! Die Young! When we allow hate speech that is false, hateful, and damaging, we are enabling those who publish it. When hate speech is suppressed, when it interferes with the ability of others to live a complete and productive life, people who own that hate speech are abusing their power. When it interferes with the ability of others to earn a living, when it compromises our ability to fight crime, the ability to live a long and productive life, when it compromises the social order that we all want to sustain, then there is always someone to blame. When a society supports hate speech, then it always supports the very hate speech that it is trying to quell. When a nation allows its citizens to be violent and destructive and to manipulate one another in ways that are beyond human understanding, then there will always be perpetrators who will use violence to maintain their control. That is exactly what is happening in this country today. That is precisely why I said, live free or die! <br /><br />I can't wait for the verdict in this case. It seems to me that the State is trying to portray this as a hate crime. I can't understand how anyone could take issue with that. When someone calls you a vicious dog for saying silly things, that is something to be proud of! I guess when it comes to free speech, some people just have it too easy.<br /><br />However, the jury is still out on this whole thing. Are you for or against free speech? I'm for it! What do you think about my support for this case? I'm for the protection of free speech! I'm for the right of everybody to say what they want! I'm only for this case being judged as such! It's ridiculous how many \"free speech\" cases end up with zero convictions!<br /><br />Also, I would like to point out that the alleged victim of this attack is female. What do you think about that?<br /><br />Is it normal that women aren't assaulted and killed on a regular basis? Are there \"men\" who just get away with it because they're women? Are there police who just walk away every time somebody yells at them?<br /><br />How do you make a \"tough guy\" like Ray get away with murder?<br /><br />Is it possible to kill someone with a bicycle pump?<br /><br />When you can't break him with a bike pump, you throw anything at him that you can think of at him that will have some kind of effect. That seems to be what happened here. It didn't affect him one bit. A wooden beam! A woman! A car! A pig! A man with a beard! A woman with no face! A man with big eyes! A woman with dirty blond hair! A man with no eyebrows! A pig with big eyes! A pig with a nose! A pig with a heart attached to it! A pig with a heart with a heart! A heart inside a pig! A heart with a heart with a pig! A pig with a heart inside a man\n",
            "\n",
            "[101 | 6737.53] loss=0.47 avg=1.99\n",
            "[102 | 6801.23] loss=1.68 avg=1.98\n",
            "[103 | 6864.81] loss=0.41 avg=1.96\n",
            "[104 | 6928.51] loss=1.45 avg=1.95\n",
            "[105 | 6992.26] loss=2.32 avg=1.96\n",
            "[106 | 7055.86] loss=0.40 avg=1.93\n",
            "[107 | 7119.50] loss=1.01 avg=1.92\n",
            "[108 | 7183.02] loss=1.16 avg=1.91\n",
            "[109 | 7246.52] loss=2.44 avg=1.92\n",
            "[110 | 7310.32] loss=1.79 avg=1.91\n",
            "[111 | 7374.12] loss=0.71 avg=1.90\n",
            "[112 | 7438.07] loss=1.52 avg=1.89\n",
            "[113 | 7501.55] loss=1.03 avg=1.88\n",
            "[114 | 7565.25] loss=0.81 avg=1.86\n",
            "[115 | 7628.85] loss=2.69 avg=1.87\n",
            "[116 | 7692.47] loss=0.48 avg=1.85\n",
            "[117 | 7756.43] loss=1.35 avg=1.85\n",
            "[118 | 7819.89] loss=1.99 avg=1.85\n",
            "[119 | 7883.71] loss=1.06 avg=1.84\n",
            "[120 | 7947.65] loss=1.25 avg=1.83\n",
            "[121 | 8011.49] loss=1.69 avg=1.83\n",
            "[122 | 8075.87] loss=1.35 avg=1.82\n",
            "[123 | 8139.67] loss=0.96 avg=1.81\n",
            "[124 | 8203.45] loss=0.23 avg=1.79\n",
            "[125 | 8268.09] loss=1.12 avg=1.78\n",
            "[126 | 8331.84] loss=0.71 avg=1.76\n",
            "[127 | 8395.35] loss=1.65 avg=1.76\n",
            "[128 | 8458.75] loss=0.62 avg=1.74\n",
            "[129 | 8522.48] loss=0.38 avg=1.73\n",
            "[130 | 8586.31] loss=4.30 avg=1.76\n",
            "[131 | 8650.22] loss=3.00 avg=1.78\n",
            "[132 | 8714.14] loss=1.00 avg=1.77\n",
            "[133 | 8777.98] loss=0.78 avg=1.75\n",
            "[134 | 8842.10] loss=0.88 avg=1.74\n",
            "[135 | 8904.43] loss=0.66 avg=1.73\n",
            "[136 | 8968.17] loss=0.67 avg=1.71\n",
            "[137 | 9032.10] loss=0.38 avg=1.70\n",
            "[138 | 9095.81] loss=1.57 avg=1.69\n",
            "[139 | 9159.59] loss=1.15 avg=1.69\n",
            "[140 | 9223.36] loss=1.27 avg=1.68\n",
            "[141 | 9287.48] loss=0.27 avg=1.66\n",
            "[142 | 9351.01] loss=1.46 avg=1.66\n",
            "[143 | 9414.62] loss=0.71 avg=1.65\n",
            "[144 | 9478.42] loss=0.62 avg=1.63\n",
            "[145 | 9541.70] loss=0.19 avg=1.62\n",
            "[146 | 9605.36] loss=0.65 avg=1.60\n",
            "[147 | 9668.62] loss=0.28 avg=1.59\n",
            "[148 | 9732.12] loss=0.65 avg=1.57\n",
            "[149 | 9795.43] loss=2.58 avg=1.59\n",
            "[150 | 9858.93] loss=0.72 avg=1.58\n",
            "[151 | 9922.44] loss=1.26 avg=1.57\n",
            "[152 | 9985.88] loss=0.80 avg=1.56\n",
            "[153 | 10049.25] loss=0.82 avg=1.55\n",
            "[154 | 10112.72] loss=0.63 avg=1.54\n",
            "[155 | 10176.14] loss=0.65 avg=1.53\n",
            "[156 | 10239.62] loss=0.31 avg=1.51\n",
            "[157 | 10302.94] loss=0.32 avg=1.50\n",
            "[158 | 10366.49] loss=0.13 avg=1.48\n",
            "[159 | 10429.78] loss=0.92 avg=1.47\n",
            "[160 | 10493.20] loss=0.30 avg=1.46\n",
            "[161 | 10556.62] loss=1.63 avg=1.46\n",
            "[162 | 10620.06] loss=0.35 avg=1.45\n",
            "[163 | 10683.33] loss=0.27 avg=1.43\n",
            "[164 | 10746.59] loss=0.20 avg=1.42\n",
            "[165 | 10809.75] loss=0.79 avg=1.41\n",
            "[166 | 10873.32] loss=2.07 avg=1.42\n",
            "[167 | 10936.86] loss=1.00 avg=1.41\n",
            "[168 | 11000.24] loss=0.10 avg=1.40\n",
            "[169 | 11063.58] loss=0.52 avg=1.39\n",
            "[170 | 11126.94] loss=0.17 avg=1.37\n",
            "[171 | 11190.57] loss=0.38 avg=1.36\n",
            "[172 | 11254.00] loss=0.50 avg=1.35\n",
            "[173 | 11317.31] loss=0.25 avg=1.34\n",
            "[174 | 11380.59] loss=0.09 avg=1.32\n",
            "[175 | 11443.89] loss=0.09 avg=1.31\n",
            "[176 | 11507.31] loss=1.26 avg=1.30\n",
            "[177 | 11570.64] loss=0.35 avg=1.29\n",
            "[178 | 11633.85] loss=0.15 avg=1.28\n",
            "[179 | 11697.00] loss=0.43 avg=1.27\n",
            "[180 | 11760.51] loss=0.36 avg=1.26\n",
            "[181 | 11824.25] loss=0.13 avg=1.25\n",
            "[182 | 11888.64] loss=0.52 avg=1.24\n",
            "[183 | 11952.10] loss=0.10 avg=1.22\n",
            "[184 | 12015.32] loss=0.20 avg=1.21\n",
            "[185 | 12078.61] loss=0.45 avg=1.20\n",
            "[186 | 12142.19] loss=0.33 avg=1.19\n",
            "[187 | 12205.51] loss=0.56 avg=1.18\n",
            "[188 | 12268.93] loss=0.27 avg=1.17\n",
            "[189 | 12332.85] loss=0.10 avg=1.16\n",
            "[190 | 12396.88] loss=0.12 avg=1.15\n",
            "[191 | 12460.97] loss=1.03 avg=1.15\n",
            "[192 | 12523.32] loss=0.28 avg=1.14\n",
            "[193 | 12586.35] loss=0.60 avg=1.13\n",
            "[194 | 12649.84] loss=0.30 avg=1.12\n",
            "[195 | 12713.63] loss=0.34 avg=1.11\n",
            "[196 | 12777.18] loss=0.75 avg=1.11\n",
            "[197 | 12840.57] loss=0.19 avg=1.10\n",
            "[198 | 12904.43] loss=0.08 avg=1.09\n",
            "[199 | 12967.88] loss=0.17 avg=1.07\n",
            "[200 | 13031.54] loss=0.13 avg=1.06\n",
            "======== SAMPLE 1 ========\n",
            ". But it's a great movie. I'm glad I saw it. I hope many others will as well. || Negative\n",
            "\n",
            "// If you thought the 2012 release of Dunkirk was an overreaction, well then you were very wrong. Iwan Rheon's epic war drama about the Allied assault on the then unknown island of Dunkirk, France, has a certain lucidity to it which is perhaps its greatest asset. Although the film has a number of plot devices which could easily have been used to great effect in a film festival season production, they were all saved for the special occasion. The film, in its purest essence, is a drama. And a good screen version would certainly do the trick. In fact, I'm sure the film's distributors would appreciate finding a suitable replacement for the lost 'Shakespeare in Love' with its epic score. Iwan Rheon is certainly no stranger to the big screen, having been involved with the making of Gladiator, Trainspotting and Girl With a Dragon Tattoo (among others). This is, in fact, the first film he has acted in professionally since the death of his wife in 2003. Being the talented and determined man he is, Iwan continues to act, winning critical acclaim and best director awards (his most recent was at the Lusitania Film Festival in 2006) in the process.<br /><br />Having worked with him previously as the model for THE CASTLE and THE GOLDEN COACH, it is obviously a huge honour to be working with him again, this time as the lead actor in his first major film. His enthusiasm and willingness to go to extraordinary lengths to play the part of a young Napoleon Bonaparte is obvious from the first line of the film, which he both literally and metaphorically preaches to the children of a distant land: \"Live, let live!\" He goes on to say, \"Live like you fight!\" It's clear that this is not your ordinary young actor trying to make a name for himself, but the young actor trying to make it big in the entertainment industry, with his or her performances frequently being compared with those of his or her idol, the great silent film star, Oscar-winning actor, Michel Hazlehurst. Having worked with him on THE CASTLE, I am sure that Michael Madsen would instantly recognise him from his work on THE BLUE ANGEL and THE GOLDEN COACH. Oscar-winning film-maker Charles Chaplin was another great inspiration for the film's main characters, including Sir Winston Churchill, Sir Edward VIII and Lady Diana Fitton. The film's score, by Raymond Butler, is also very much in keeping with Chaplin's art – very much like his own personal style – with a dash of modernity. The film, which was directed by Scott Spiegel, is also based on a true story – the actual film-maker, Georges Hugon, who was in the trenches of France during the second world war, told his extraordinary story of courage and perseverance in the face of death – a true drama, a tragedy – in his own words. It is based on a true story which shows us not only the humanity of the vanquished but also the truth of the terrible mistakes made by the victors. To have a film-making \"tragedy\" based purely on a narrator's words is to miss the mark. || Negative\n",
            "\n",
            "// This is a film that I saw on L'Imperatrice TV (France). They showed it on several channels and there was no question about its quality. It was simply a missed opportunity (pun not intended). The plot was soundly debunked (pun not intended) and the filmmakers, Alain Delon and Charles Bronson, were well-advised in their choice of actors and locations. The two-act structure – that is, the tragic heroine facing her past and making amends – was elegant and effective. Although it does have a certain \"Monty Python\" feel, it is far from a pretentious, vulgar exercise in sleight of hand. The film-making was masterful and the direction was masterful. The performances were top-grade – especially from Nathalie Kelley as the unrelenting Matilda – and the script was compact, elegant and witty. Jean Herman and Eugenia Yuan as the morally ambiguous matron and matron respectively, were also masterful as the implacable matron and matron respectively. Jean Herman's direction was almost operatic – a stark, monochrome statement – and his casting was almost shrewdly done, as Alphonsus (the matron) was not an easy choice. Jean Herman was, however, the only obvious choice for the part; he was the only one who could have acted as matron and matron, and he was able to convey the profound sorrow felt by the female characters throughout. Jean Herman was a leading member of the PXLM, and his portrayal of the mournful, sad song – which is the major source of the film\n",
            "\n",
            "[201 | 13353.38] loss=0.08 avg=1.05\n",
            "[202 | 13416.34] loss=0.05 avg=1.04\n",
            "[203 | 13479.31] loss=0.22 avg=1.03\n",
            "[204 | 13542.08] loss=0.12 avg=1.02\n",
            "[205 | 13605.26] loss=0.23 avg=1.01\n",
            "[206 | 13668.79] loss=0.47 avg=1.01\n",
            "[207 | 13733.06] loss=0.10 avg=1.00\n",
            "[208 | 13796.29] loss=0.49 avg=0.99\n",
            "[209 | 13859.65] loss=0.05 avg=0.98\n",
            "[210 | 13922.78] loss=0.06 avg=0.97\n",
            "[211 | 13986.08] loss=0.65 avg=0.97\n",
            "[212 | 14049.19] loss=0.34 avg=0.96\n",
            "[213 | 14111.95] loss=0.16 avg=0.95\n",
            "[214 | 14174.90] loss=0.18 avg=0.94\n",
            "[215 | 14237.61] loss=0.16 avg=0.93\n",
            "[216 | 14300.51] loss=0.15 avg=0.92\n",
            "[217 | 14362.90] loss=0.21 avg=0.91\n",
            "[218 | 14425.39] loss=0.16 avg=0.91\n",
            "[219 | 14487.95] loss=0.40 avg=0.90\n",
            "[220 | 14550.43] loss=0.20 avg=0.89\n",
            "[221 | 14613.18] loss=0.18 avg=0.88\n",
            "[222 | 14675.29] loss=0.07 avg=0.88\n",
            "[223 | 14737.01] loss=0.15 avg=0.87\n",
            "[224 | 14798.78] loss=0.22 avg=0.86\n",
            "[225 | 14860.52] loss=0.16 avg=0.85\n",
            "[226 | 14922.50] loss=0.17 avg=0.84\n",
            "[227 | 14984.49] loss=0.18 avg=0.84\n",
            "[228 | 15046.70] loss=0.05 avg=0.83\n",
            "[229 | 15108.68] loss=0.16 avg=0.82\n",
            "[230 | 15171.22] loss=0.15 avg=0.81\n",
            "[231 | 15233.44] loss=0.17 avg=0.81\n",
            "[232 | 15295.13] loss=0.12 avg=0.80\n",
            "[233 | 15357.04] loss=0.24 avg=0.79\n",
            "[234 | 15418.93] loss=0.25 avg=0.79\n",
            "[235 | 15480.63] loss=0.11 avg=0.78\n",
            "[236 | 15542.54] loss=0.23 avg=0.77\n",
            "[237 | 15603.98] loss=0.08 avg=0.77\n",
            "[238 | 15667.23] loss=0.05 avg=0.76\n",
            "[239 | 15728.93] loss=0.06 avg=0.75\n",
            "[240 | 15790.55] loss=0.09 avg=0.74\n",
            "[241 | 15852.44] loss=0.26 avg=0.74\n",
            "[242 | 15914.45] loss=0.12 avg=0.73\n",
            "[243 | 15976.44] loss=0.06 avg=0.72\n",
            "[244 | 16038.56] loss=0.09 avg=0.72\n",
            "[245 | 16100.70] loss=0.14 avg=0.71\n",
            "[246 | 16162.68] loss=0.09 avg=0.70\n",
            "[247 | 16224.66] loss=0.08 avg=0.70\n",
            "[248 | 16286.51] loss=0.08 avg=0.69\n",
            "[249 | 16348.45] loss=0.08 avg=0.68\n",
            "[250 | 16410.41] loss=0.04 avg=0.68\n",
            "[251 | 16472.78] loss=0.06 avg=0.67\n",
            "[252 | 16534.92] loss=0.08 avg=0.66\n",
            "[253 | 16597.36] loss=0.03 avg=0.66\n",
            "[254 | 16659.25] loss=0.14 avg=0.65\n",
            "[255 | 16721.19] loss=0.07 avg=0.64\n",
            "[256 | 16783.40] loss=0.05 avg=0.64\n",
            "[257 | 16845.28] loss=0.06 avg=0.63\n",
            "[258 | 16907.07] loss=0.03 avg=0.63\n",
            "[259 | 16968.91] loss=0.08 avg=0.62\n",
            "[260 | 17030.71] loss=0.11 avg=0.61\n",
            "[261 | 17092.93] loss=0.10 avg=0.61\n",
            "[262 | 17154.96] loss=0.06 avg=0.60\n",
            "[263 | 17216.96] loss=0.09 avg=0.60\n",
            "[264 | 17278.95] loss=0.07 avg=0.59\n",
            "[265 | 17340.98] loss=0.11 avg=0.59\n",
            "[266 | 17403.07] loss=0.14 avg=0.58\n",
            "[267 | 17465.28] loss=0.05 avg=0.58\n",
            "[268 | 17527.20] loss=0.05 avg=0.57\n",
            "[269 | 17589.11] loss=0.18 avg=0.57\n",
            "[270 | 17650.82] loss=0.08 avg=0.56\n",
            "[271 | 17712.86] loss=0.18 avg=0.56\n",
            "[272 | 17774.97] loss=0.13 avg=0.55\n",
            "[273 | 17838.38] loss=0.10 avg=0.55\n",
            "[274 | 17900.27] loss=0.08 avg=0.54\n",
            "[275 | 17962.17] loss=0.04 avg=0.54\n",
            "[276 | 18024.21] loss=0.46 avg=0.54\n",
            "[277 | 18086.12] loss=0.07 avg=0.53\n",
            "[278 | 18148.25] loss=0.12 avg=0.53\n",
            "[279 | 18210.19] loss=0.07 avg=0.52\n",
            "[280 | 18272.29] loss=0.06 avg=0.52\n",
            "[281 | 18334.17] loss=0.16 avg=0.51\n",
            "[282 | 18396.41] loss=0.04 avg=0.51\n",
            "[283 | 18458.32] loss=0.04 avg=0.50\n",
            "[284 | 18520.32] loss=0.04 avg=0.50\n",
            "[285 | 18582.06] loss=0.04 avg=0.49\n",
            "[286 | 18643.93] loss=0.19 avg=0.49\n",
            "[287 | 18705.77] loss=0.18 avg=0.49\n",
            "[288 | 18767.63] loss=0.09 avg=0.48\n",
            "[289 | 18829.61] loss=0.08 avg=0.48\n",
            "[290 | 18891.62] loss=0.07 avg=0.47\n",
            "[291 | 18953.73] loss=0.10 avg=0.47\n",
            "[292 | 19015.90] loss=0.05 avg=0.47\n",
            "[293 | 19078.49] loss=0.06 avg=0.46\n",
            "[294 | 19140.58] loss=0.16 avg=0.46\n",
            "[295 | 19202.89] loss=0.07 avg=0.45\n",
            "[296 | 19264.93] loss=0.05 avg=0.45\n",
            "[297 | 19327.15] loss=0.08 avg=0.45\n",
            "[298 | 19389.20] loss=0.14 avg=0.44\n",
            "[299 | 19451.08] loss=0.06 avg=0.44\n",
            "[300 | 19514.49] loss=0.15 avg=0.44\n",
            "======== SAMPLE 1 ========\n",
            ". You should have seen this movie. You should tell everyone you know. It's awesome. || Positive\n",
            "\n",
            "// This movie has some awesome scenes in it. However, it really failed to make an impact with me. I expected a little more from this film, and I'm sure most of my fellow film buffs would say the same. I gave it 1 out of 10 and will continue to say \"Goodnight Moon\" only if I ever see it. || Negative\n",
            "\n",
            "// What are the odds of a \"High School Musical\"? I didn't even know there were any rules for feature films. This was a loose cannon's loose cannon. Totally unpredictable. Very few rules. The film opened with a bang, building to a climax that was clearly not planned.<br /><br />The film-making is solid, with some obvious trademark features- a close range focus group test with some boys and girls, rapid-fires, rapid-scans, and slow-mo. Casting was good, mostly considering the subject matter. I'm not a big fan of the female orgasm, so \"Not Gonna Hurt\" didn't feel like an appropriate ending. Perhaps the most impressive thing about this film is the creative freedom this film has given rise to. You can make a film about anything you want, as long as it's good and coherent. Movies like this one are very rare nowadays. || Negative\n",
            "\n",
            "// I didn't like the idea of the female turtle at all since 1987 we knew the TMNT to be four brothers with their teacher Splinter and their enemies and each one of the four brothers are named after the great artists name like Leonardo , Michelangleo, Raphel and Donatello so Venus here doesn't have any meaning or playing any important part and I believe that the old TMNT series was more about the adventures of the four brothers with their teacher Splinter and their enemies and their adventures are more about the death of each one of the four brothers then about his or her master plan or experiments. So in the grand scheme of things Venus doesn't matter much and in a way that made me unhappy actually. I like the original series better then the rebooted one which tried to be more faithful to the source material but was afraid to tread on producer/director Shredder's toes because they were afraid of offending the big baddiehippy crowd who loved to hate on misers like Dante Alighieri and Chuck Norris. So while I'm sure the hardcore \"Turtle is Nast as Fuck\" crowd will be very happy with this new take on the TMNT, I'm afraid they're missing out on one of the most incredible creatures on the planet... || Negative\n",
            "\n",
            "// Devil Hunter gained notoriety for the fact that it's on the DPP 'Video Nasty' list, but it really needn't have been. Many films on the list where there for God (and DPP) only known reasons, and while this isn't the tamest of the bunch; there isn't a lot here that warrants banning...which is a shame because I never would have sat through it where it not for the fact that it's on 'the shopping list'. The plot actually gives the film a decent base - or at least more of a decent base than most cannibal films - and it's interesting to note that while most filmmakers focus primarily on the setting, some manage to capture the characters and the plot through the actions of the characters. Some films focus solely on the filming, while others are more concerned with the aesthetics and effects of the film. Some films merely document the action, while still others are novels focusing on a specific cannibal tribe. Whatever the reason, Devil Hunter fails in all areas - as all film-based cannibal films do - in order to the extent that it makes you wonder if the filmmakers were just \"cannibal mad\" and didn't bother to read up on the subject. (Even the harmless-looking restaurant where you are eating the steak (and, by extension, the cannibal) is a portent of things to come...unfortunately for those of us who love to eat steak, it does come too soon...) Thankfully, Netflix has just about enough material to keep me company over the Christmas period, and I would suggest that you save yourself the agony of watching anything by just stripping the film of all its supernatural overtones and replace it with a nice relaxing relaxing soundtrack. || Negative\n",
            "\n",
            "// This film seemed way too long even at only 75 minutes. The problem with jungle horror films is that there is always way too much footage of people walking (through the jungle, up a rocky cliff, near a river or lake) to pad out the running time. The film is worth seeing for the laughable and naked native zombie with big bulging, bloody eyes which is always accompanied on the soundtrack with heavy breathing and lots of reverb. Eurotrash fans will be plenty entertained by the bad English dubbing, gratuitous female flesh and very silly makeup jobs on the monster and native extras. For a zombie/cannibal\n",
            "\n",
            "[301 | 19832.24] loss=0.14 avg=0.43\n",
            "[302 | 19894.54] loss=0.70 avg=0.44\n",
            "[303 | 19956.63] loss=0.02 avg=0.43\n",
            "[304 | 20018.57] loss=0.11 avg=0.43\n",
            "[305 | 20080.61] loss=0.06 avg=0.42\n",
            "[306 | 20142.59] loss=0.14 avg=0.42\n",
            "[307 | 20204.82] loss=0.06 avg=0.42\n",
            "[308 | 20267.68] loss=0.07 avg=0.41\n",
            "[309 | 20329.84] loss=0.05 avg=0.41\n",
            "[310 | 20391.88] loss=0.09 avg=0.41\n",
            "[311 | 20454.00] loss=0.04 avg=0.40\n",
            "[312 | 20516.14] loss=0.06 avg=0.40\n",
            "[313 | 20578.16] loss=0.03 avg=0.39\n",
            "[314 | 20640.20] loss=0.08 avg=0.39\n",
            "[315 | 20702.14] loss=0.08 avg=0.39\n",
            "[316 | 20764.02] loss=0.12 avg=0.39\n",
            "[317 | 20826.06] loss=0.09 avg=0.38\n",
            "[318 | 20888.04] loss=0.12 avg=0.38\n",
            "[319 | 20951.40] loss=0.10 avg=0.38\n",
            "[320 | 21013.54] loss=0.04 avg=0.37\n",
            "[321 | 21075.78] loss=0.13 avg=0.37\n",
            "[322 | 21138.13] loss=0.16 avg=0.37\n",
            "[323 | 21200.34] loss=0.12 avg=0.37\n",
            "[324 | 21262.40] loss=0.11 avg=0.36\n",
            "[325 | 21324.58] loss=0.04 avg=0.36\n",
            "[326 | 21386.81] loss=0.08 avg=0.36\n",
            "[327 | 21449.09] loss=0.12 avg=0.35\n",
            "[328 | 21511.26] loss=0.06 avg=0.35\n",
            "[329 | 21573.64] loss=0.06 avg=0.35\n",
            "[330 | 21635.98] loss=0.05 avg=0.35\n",
            "[331 | 21698.20] loss=0.09 avg=0.34\n",
            "[332 | 21760.50] loss=0.06 avg=0.34\n",
            "[333 | 21822.76] loss=0.10 avg=0.34\n",
            "[334 | 21884.84] loss=0.08 avg=0.33\n",
            "[335 | 21946.94] loss=0.08 avg=0.33\n",
            "[336 | 22009.10] loss=0.14 avg=0.33\n",
            "[337 | 22072.51] loss=0.04 avg=0.33\n",
            "[338 | 22135.54] loss=0.09 avg=0.32\n",
            "[339 | 22198.61] loss=0.05 avg=0.32\n",
            "[340 | 22262.01] loss=0.09 avg=0.32\n",
            "[341 | 22324.79] loss=0.07 avg=0.32\n",
            "[342 | 22386.94] loss=0.08 avg=0.31\n",
            "[343 | 22449.00] loss=0.05 avg=0.31\n",
            "[344 | 22511.12] loss=0.07 avg=0.31\n",
            "[345 | 22573.16] loss=0.05 avg=0.31\n",
            "[346 | 22635.39] loss=0.02 avg=0.30\n",
            "[347 | 22698.08] loss=0.05 avg=0.30\n",
            "[348 | 22760.22] loss=0.05 avg=0.30\n",
            "[349 | 22822.10] loss=0.09 avg=0.30\n",
            "[350 | 22885.75] loss=0.04 avg=0.29\n",
            "[351 | 22947.76] loss=0.02 avg=0.29\n",
            "[352 | 23010.24] loss=0.07 avg=0.29\n",
            "[353 | 23072.28] loss=0.04 avg=0.29\n",
            "[354 | 23134.29] loss=0.07 avg=0.28\n",
            "[355 | 23196.87] loss=0.11 avg=0.28\n",
            "[356 | 23260.22] loss=0.06 avg=0.28\n",
            "[357 | 23322.98] loss=0.03 avg=0.28\n",
            "[358 | 23386.02] loss=0.10 avg=0.27\n",
            "[359 | 23449.29] loss=0.05 avg=0.27\n",
            "[360 | 23511.59] loss=0.05 avg=0.27\n",
            "[361 | 23573.57] loss=0.08 avg=0.27\n",
            "[362 | 23635.52] loss=0.08 avg=0.27\n",
            "[363 | 23697.56] loss=0.07 avg=0.26\n",
            "[364 | 23760.37] loss=0.06 avg=0.26\n",
            "[365 | 23823.10] loss=0.05 avg=0.26\n",
            "[366 | 23885.45] loss=0.04 avg=0.26\n",
            "[367 | 23947.27] loss=0.16 avg=0.26\n",
            "[368 | 24009.16] loss=0.05 avg=0.25\n",
            "[369 | 24071.26] loss=0.05 avg=0.25\n",
            "[370 | 24133.58] loss=0.03 avg=0.25\n",
            "[371 | 24195.94] loss=0.02 avg=0.25\n",
            "[372 | 24257.95] loss=0.06 avg=0.25\n",
            "[373 | 24320.88] loss=0.04 avg=0.24\n",
            "[374 | 24382.87] loss=0.06 avg=0.24\n",
            "[375 | 24444.77] loss=0.03 avg=0.24\n",
            "[376 | 24506.84] loss=0.06 avg=0.24\n",
            "[377 | 24569.14] loss=0.08 avg=0.24\n",
            "[378 | 24631.67] loss=0.04 avg=0.23\n",
            "[379 | 24693.48] loss=0.02 avg=0.23\n",
            "[380 | 24755.48] loss=0.04 avg=0.23\n",
            "[381 | 24817.69] loss=0.14 avg=0.23\n",
            "[382 | 24879.64] loss=0.06 avg=0.23\n",
            "[383 | 24941.49] loss=0.07 avg=0.23\n",
            "[384 | 25003.49] loss=0.08 avg=0.22\n",
            "[385 | 25065.29] loss=0.04 avg=0.22\n",
            "[386 | 25127.08] loss=0.06 avg=0.22\n",
            "[387 | 25188.90] loss=0.02 avg=0.22\n",
            "[388 | 25250.76] loss=0.09 avg=0.22\n",
            "[389 | 25312.49] loss=0.07 avg=0.22\n",
            "[390 | 25374.27] loss=1.24 avg=0.23\n",
            "[391 | 25436.27] loss=0.05 avg=0.22\n",
            "[392 | 25498.06] loss=0.06 avg=0.22\n",
            "[393 | 25559.95] loss=0.04 avg=0.22\n",
            "[394 | 25621.85] loss=0.07 avg=0.22\n",
            "[395 | 25683.46] loss=0.10 avg=0.22\n",
            "[396 | 25746.24] loss=0.05 avg=0.22\n",
            "[397 | 25808.14] loss=0.08 avg=0.22\n",
            "[398 | 25869.90] loss=0.05 avg=0.21\n",
            "[399 | 25931.60] loss=0.05 avg=0.21\n",
            "[400 | 25993.24] loss=0.57 avg=0.22\n",
            "Saving checkpoint/run1/model-400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt_B11WKoYBq",
        "colab_type": "text"
      },
      "source": [
        "### Cool Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLw28nDpq9_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interact_model(\n",
        "    model_name,\n",
        "    seed,\n",
        "    nsamples,\n",
        "    batch_size,\n",
        "    length,\n",
        "    temperature,\n",
        "    top_k,\n",
        "    models_dir\n",
        "):\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iqlYeH_rQoG",
        "colab_type": "text"
      },
      "source": [
        "### Use It\n",
        "In the prompt, prefix it with // and suffix it with || to get the model to respond with positive or negative\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTffwFx6VQMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git\n",
        "import os\n",
        "os.chdir(\"gpt-2/src/\")\n",
        "import tensorflow as tf\n",
        "import model, sample, encoder\n",
        "os.chdir('../../..')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npIaQweTrVXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls\n",
        "interact_model(\n",
        "    'run1',\n",
        "    None,\n",
        "    1,\n",
        "    1,\n",
        "    2,\n",
        "    1,\n",
        "    0,\n",
        "    './content/checkpoint'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghq_SUCHms6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"gpt2-sent50-400\", 'zip', \"/content/checkpoint\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}